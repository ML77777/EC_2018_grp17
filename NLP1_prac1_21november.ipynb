{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of NLP1 Practical I (student version)",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML77777/EC_2018_grp17/blob/master/NLP1_prac1_21november.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "lIZrAUx57vsM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Practical 1: Sentiment Detection of Movie Reviews\n",
        "========================================\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "J4kXPMhyngZW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This practical concerns sentiment detection of movie reviews.\n",
        "In [this file](https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json) (80MB) you will find 1000 positive and 1000 negative **movie reviews**.\n",
        "Each review is a **document** and consists of one or more sentences.\n",
        "\n",
        "To prepare yourself for this practical, you should\n",
        "have a look at a few of these texts to understand the difficulties of\n",
        "the task (how might one go about classifying the texts?); you will write\n",
        "code that decides whether a random unseen movie review is positive or\n",
        "negative.\n",
        "\n",
        "Please make sure you have read the following paper:\n",
        "\n",
        ">   Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan\n",
        "(2002). \n",
        "[Thumbs up? Sentiment Classification using Machine Learning\n",
        "Techniques](https://dl.acm.org/citation.cfm?id=1118704). EMNLP.\n",
        "\n",
        "Bo Pang et al. were the \"inventors\" of the movie review sentiment\n",
        "classification task, and the above paper was one of the first papers on\n",
        "the topic. The first version of your sentiment classifier will do\n",
        "something similar to Bo Pang’s system. If you have questions about it,\n",
        "we should resolve them in our first demonstrated practical.\n"
      ]
    },
    {
      "metadata": {
        "id": "cb7errgRASzZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Advice**\n",
        "\n",
        "Please read through the entire practical and familiarise\n",
        "yourself with all requirements before you start coding or otherwise\n",
        "solving the tasks. Writing clean and concise code can make the difference\n",
        "between solving the assignment in a matter of hours, and taking days to\n",
        "run all experiments.\n",
        "\n",
        "**Environment**\n",
        "\n",
        "All code should be written in **Python 3**. \n",
        "If you use Colab, check if you have that version with `Runtime -> Change runtime type` in the top menu.\n",
        "\n",
        "> If you want to work in your own computer, then download this notebook through `File -> Download .ipynb`.\n",
        "The easiest way to\n",
        "install Python is through downloading\n",
        "[Anaconda](https://www.anaconda.com/download). \n",
        "After installation, you can start the notebook by typing `jupyter notebook filename.ipynb`.\n",
        "You can also use an IDE\n",
        "such as [PyCharm](https://www.jetbrains.com/pycharm/download/) to make\n",
        "coding and debugging easier. It is good practice to create a [virtual\n",
        "environment](https://docs.python.org/3/tutorial/venv.html) for this\n",
        "project, so that any Python packages don’t interfere with other\n",
        "projects.\n",
        "\n",
        "#### Learning Python 3\n",
        "\n",
        "If you are new to Python 3, you may want to check out a few of these resources:\n",
        "- https://learnxinyminutes.com/docs/python3/\n",
        "- https://www.learnpython.org/\n",
        "- https://docs.python.org/3/tutorial/"
      ]
    },
    {
      "metadata": {
        "id": "bXWyGHwE-ieQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Loading the Data\n",
        "-------------------------------------------------------------"
      ]
    },
    {
      "metadata": {
        "id": "lm-rakqtlMOT",
        "colab_type": "code",
        "outputId": "84b16c43-dbe4-4ae7-db29-d5261e3b69ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "cell_type": "code",
      "source": [
        "# download sentiment lexicon\n",
        "!wget https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6/sent_lexicon\n",
        "# download review data\n",
        "!wget https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-11-21 19:22:25--  https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6/sent_lexicon\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 662577 (647K) [text/plain]\n",
            "Saving to: ‘sent_lexicon’\n",
            "\n",
            "\rsent_lexicon          0%[                    ]       0  --.-KB/s               \rsent_lexicon        100%[===================>] 647.05K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2018-11-21 19:22:25 (10.3 MB/s) - ‘sent_lexicon’ saved [662577/662577]\n",
            "\n",
            "--2018-11-21 19:22:26--  https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 83503869 (80M) [text/plain]\n",
            "Saving to: ‘reviews.json’\n",
            "\n",
            "reviews.json        100%[===================>]  79.63M  94.6MB/s    in 0.8s    \n",
            "\n",
            "2018-11-21 19:22:28 (94.6 MB/s) - ‘reviews.json’ saved [83503869/83503869]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hok-BFu9lGoK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import sys\n",
        "from subprocess import call\n",
        "from nltk import FreqDist\n",
        "from nltk.util import ngrams\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import sklearn as sk\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "import json\n",
        "from collections import Counter\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "careEKj-mRpl",
        "colab_type": "code",
        "outputId": "837c87de-b744-4eab-ef8d-5a584f1e8bbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "cell_type": "code",
      "source": [
        "# load reviews into memory\n",
        "# file structure:\n",
        "# [\n",
        "#  {\"cv\": integer, \"sentiment\": str, \"content\": list} \n",
        "#  {\"cv\": integer, \"sentiment\": str, \"content\": list} \n",
        "#   ..\n",
        "# ]\n",
        "# where `content` is a list of sentences, \n",
        "# with a sentence being a list of (token, pos_tag) pairs.\n",
        "\n",
        "# For documentation on POS-tags, see \n",
        "# https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf\n",
        "\n",
        "with open(\"reviews.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  reviews = json.load(f)\n",
        "  \n",
        "print(len(reviews))\n",
        "\n",
        "def print_sentence_with_pos(s):\n",
        "  print(\" \".join(\"%s/%s\" % (token, pos_tag) for token, pos_tag in s))\n",
        "\n",
        "for i, r in enumerate(reviews):\n",
        "  print(r[\"cv\"], r[\"sentiment\"], len(r[\"content\"]))  # cv, sentiment, num sents\n",
        "  print_sentence_with_pos(r[\"content\"][0])\n",
        "  if i == 4: \n",
        "    break\n",
        "    \n",
        "c = Counter()\n",
        "for review in reviews:\n",
        "  for sentence in review[\"content\"]:\n",
        "    for token, pos_tag in sentence:\n",
        "      c[token.lower()] += 1\n",
        "      \n",
        "print(\"#types\", len(c))\n",
        "\n",
        "print(\"Most common tokens:\")\n",
        "for token, count in c.most_common(25):\n",
        "  print(\"%10s : %8d\" % (token, count))\n",
        "  "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n",
            "0 NEG 29\n",
            "Two/CD teen/JJ couples/NNS go/VBP to/TO a/DT church/NN party/NN ,/, drink/NN and/CC then/RB drive/NN ./.\n",
            "1 NEG 11\n",
            "Damn/JJ that/IN Y2K/CD bug/NN ./.\n",
            "2 NEG 24\n",
            "It/PRP is/VBZ movies/NNS like/IN these/DT that/WDT make/VBP a/DT jaded/JJ movie/NN viewer/NN thankful/JJ for/IN the/DT invention/NN of/IN the/DT Timex/NNP IndiGlo/NNP watch/NN ./.\n",
            "3 NEG 19\n",
            "QUEST/NN FOR/IN CAMELOT/NNP ``/`` Quest/NNP for/IN Camelot/NNP ''/'' is/VBZ Warner/NNP Bros./NNP '/POS first/JJ feature-length/JJ ,/, fully-animated/JJ attempt/NN to/TO steal/VB clout/NN from/IN Disney/NNP 's/POS cartoon/NN empire/NN ,/, but/CC the/DT mouse/NN has/VBZ no/DT reason/NN to/TO be/VB worried/VBN ./.\n",
            "4 NEG 38\n",
            "Synopsis/NNPS :/: A/DT mentally/RB unstable/JJ man/NN undergoing/VBG psychotherapy/NN saves/VBZ a/DT boy/NN from/IN a/DT potentially/RB fatal/JJ accident/NN and/CC then/RB falls/VBZ in/IN love/NN with/IN the/DT boy/NN 's/POS mother/NN ,/, a/DT fledgling/NN restauranteur/NN ./.\n",
            "#types 47743\n",
            "Most common tokens:\n",
            "         , :    77842\n",
            "       the :    75948\n",
            "         . :    59027\n",
            "         a :    37583\n",
            "       and :    35235\n",
            "        of :    33864\n",
            "        to :    31601\n",
            "        is :    25972\n",
            "        in :    21563\n",
            "        's :    18043\n",
            "        it :    15904\n",
            "      that :    15820\n",
            "     -rrb- :    11768\n",
            "     -lrb- :    11670\n",
            "        as :    11312\n",
            "      with :    10739\n",
            "       for :     9816\n",
            "       his :     9542\n",
            "      this :     9497\n",
            "      film :     9404\n",
            "        '' :     9282\n",
            "        he :     8804\n",
            "        `` :     8801\n",
            "         i :     8619\n",
            "       but :     8537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E6PWaEoh8B34",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Symbolic approach – sentiment lexicon (2pts)\n",
        "---------------------------------------------------------------------\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "JsTSMb6ma4E8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**How** could one automatically classify movie reviews according to their\n",
        "sentiment? \n",
        "\n",
        "If we had access to a **sentiment lexicon**, then there are ways to solve\n",
        "the problem without using Machine Learning. One might simply look up\n",
        "every open-class word in the lexicon, and compute a binary score\n",
        "$S_{binary}$ by counting how many words match either a positive, or a\n",
        "negative word entry in the sentiment lexicon $SLex$.\n",
        "\n",
        "$$S_{binary}(w_1w_2...w_n) = \\sum_{i = 1}^{n}\\text{sgn}(SLex\\big[w_i\\big])$$\n",
        "\n",
        "**Threshold.** In average there are more positive than negative words per review (~7.13 more positive than negative per review) to take this bias into account you should use a threshold of **8** (roughly the bias itself) to make it harder to classify as positive.\n",
        "\n",
        "$$\n",
        "\\text{classify}(S_{binary}(w_1w_2...w_n)) = \\bigg\\{\\begin{array}{ll}\n",
        "        \\text{positive} & \\text{if } S_{binary}(w_1w_2...w_n) > threshold\\\\\n",
        "        \\text{negative} & \\text{else }\n",
        "        \\end{array}\n",
        "$$\n",
        "\n",
        "To implement this approach, you should use the sentiment\n",
        "lexicon in `sent_lexicon`, which was taken from the\n",
        "following work:\n",
        "\n",
        "> Theresa Wilson, Janyce Wiebe, and Paul Hoffmann\n",
        "(2005). [Recognizing Contextual Polarity in Phrase-Level Sentiment\n",
        "Analysis](http://www.aclweb.org/anthology/H/H05/H05-1044.pdf). HLT-EMNLP."
      ]
    },
    {
      "metadata": {
        "id": "tOFnMvbeeZrc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### (Q: 1.1) Implement this approach and report its classification accuracy. (1 pt)"
      ]
    },
    {
      "metadata": {
        "id": "ED2aTEYutW1-",
        "colab_type": "code",
        "outputId": "f07e3a36-8d01-458c-8b8e-916656e6b5bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "slex = open(\"sent_lexicon\",\"r\").readlines()\n",
        "\n",
        "def create_lookup(slex):\n",
        "    lookup_table = {}\n",
        "    #distinct_pos_tags = []\n",
        "    for i in slex:\n",
        "        dict_element = i.split()\n",
        "        word = dict_element[2].split(\"=\")[1]\n",
        "        pos_tag = dict_element[3].split(\"=\")[1]\n",
        "        polarity =  dict_element[5].split(\"=\")[1]\n",
        "        #lookup_table[(word,pos_tag)] = polarity\n",
        "        lookup_table[word] = polarity\n",
        "        #if pos_tag not in distinct_pos_tags:\n",
        "        #    distinct_pos_tags.append(pos_tag) #Possible tags in lexicon are, adj (JJ), noun (NN), adverb (RB) and anypos\n",
        "    \n",
        "    return lookup_table\n",
        "\n",
        "def predict_review_binary(lookup_table,list_sentences):\n",
        "    result = \"NEG\"\n",
        "    sentiment_score = 0\n",
        "    for sentence in list_sentences:\n",
        "        for token,pos_tag in sentence:\n",
        "            #print(token,pos_tag)\n",
        "            if \"JJ\" in pos_tag:\n",
        "                pos_tag_key = \"adj\"\n",
        "            elif \"NN\" in pos_tag:\n",
        "                pos_tag_key = \"noun\"\n",
        "            elif \"RB\" in pos_tag:\n",
        "                pos_tag_key = \"adverb\"\n",
        "            else: pos_tag_key = \"anypos\"\n",
        "            #key = (token,pos_tag_key) #64.05 accuracy\n",
        "            #key = (token.lower(),pos_tag_key) #62.9 accuracy\n",
        "            #key = token.lower() #67.75 accuracy\n",
        "            key = token #67.95 accuracy\n",
        "            if key in lookup_table:\n",
        "                polarity = lookup_table[key]\n",
        "                if polarity == \"positive\":\n",
        "                    sentiment_score += 1\n",
        "                elif polarity == \"negative\":\n",
        "                    sentiment_score -= 1\n",
        "    if sentiment_score > 8:\n",
        "        result = \"POS\"\n",
        "    #print(\"Sentiment score: {}\".format(sentiment_score))\n",
        "    return result\n",
        "\n",
        "def get_binary_results(lookup_table, reviews):\n",
        "    targets = []\n",
        "    predictions = []\n",
        "    correctness_predictions = []\n",
        "    for review in reviews: \n",
        "        prediction = predict_review_binary(lookup_table,review[\"content\"])\n",
        "        predictions.append(prediction)\n",
        "        target = review[\"sentiment\"]\n",
        "        targets.append(target)\n",
        "        if prediction == target:\n",
        "            correctness_predictions.append(\"+\")\n",
        "        else: correctness_predictions.append(\"-\")\n",
        "    return (predictions,targets,correctness_predictions)\n",
        "\n",
        "def calc_accuracy(predictions,targets):\n",
        "    amount_correct = 0\n",
        "    amount_reviews = len(targets)\n",
        "\n",
        "    for i in range(amount_reviews):\n",
        "        prediction = predictions[i]\n",
        "        target = targets[i]\n",
        "        if prediction == target:\n",
        "            amount_correct += 1\n",
        "    accuracy = amount_correct/amount_reviews #* 100\n",
        "    return accuracy\n",
        "\n",
        "lookup_table = create_lookup(slex) \n",
        "predictions,targets,correctness_results = get_binary_results(lookup_table,reviews)\n",
        "accuracy = calc_accuracy(predictions,targets)\n",
        "print(\"Accuracy: {}\".format(accuracy))"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iy528EUTphz5",
        "colab_type": "code",
        "outputId": "75245e11-f82d-4897-b807-b2a370596cca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "token_results = correctness_results # Correctness of sentiment predictons against the actual targets\n",
        "token_accuracy = accuracy # 67.95 for just comparing words, taking pos_tag into account drops to 64 or 62%\n",
        "print(\"Accuracy: %0.2f\" % token_accuracy)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 67.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Twox0s_3eS0V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If the sentiment lexicon also has information about the **magnitude** of\n",
        "sentiment (e.g., *“excellent\"* would have higher magnitude than\n",
        "*“good\"*), we could take a more fine-grained approach by adding up all\n",
        "sentiment scores, and deciding the polarity of the movie review using\n",
        "the sign of the weighted score $S_{weighted}$.\n",
        "\n",
        "$$S_{weighted}(w_1w_2...w_n) = \\sum_{i = 1}^{n}SLex\\big[w_i\\big]$$\n",
        "\n",
        "\n",
        "Their lexicon also records two possible magnitudes of sentiment (*weak*\n",
        "and *strong*), so you can implement both the binary and the weighted\n",
        "solutions (please use a switch in your program). For the weighted\n",
        "solution, you can choose the weights intuitively *once* before running\n",
        "the experiment.\n",
        "\n",
        "#### (Q: 1.2) Now incorporate magnitude information and report the classification accuracy. Don't forget to use the threshold. (1 pt)"
      ]
    },
    {
      "metadata": {
        "id": "qG3hUDnPtkhS",
        "colab_type": "code",
        "outputId": "8f91158b-abd0-4596-eeb0-9605d1782b39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "def create_lookup_extension(slex,weighted=False):\n",
        "    lookup_table = {}\n",
        "    #distinct_pos_tags = []\n",
        "    for i in slex:\n",
        "        dict_element = i.split()\n",
        "        word = dict_element[2].split(\"=\")[1]\n",
        "        pos_tag = dict_element[3].split(\"=\")[1]\n",
        "        polarity =  dict_element[5].split(\"=\")[1]\n",
        "        #lookup_table[(word,pos_tag)] = polarity\n",
        "        #Extension for weighted solution\n",
        "        if weighted:\n",
        "            type_subj = dict_element[0].split(\"=\")[1]\n",
        "            lookup_table[word] = (polarity,type_subj)\n",
        "        else: \n",
        "            lookup_table[word] = polarity\n",
        "        #if pos_tag not in distinct_pos_tags:\n",
        "        #    distinct_pos_tags.append(pos_tag) #Possible tags in lexicon are, adj (JJ), noun (NN), adverb (RB) and anypos\n",
        "    \n",
        "    return lookup_table\n",
        "\n",
        "def predict_review_weighted(lookup_table,list_sentences, weight_argument):\n",
        "    result = \"NEG\"\n",
        "    sentiment_score = 0\n",
        "    for sentence in list_sentences:\n",
        "        for token,pos_tag in sentence:\n",
        "            key = token \n",
        "            if key in lookup_table:\n",
        "                polarity,type_subj = lookup_table[key]\n",
        "                if type_subj == \"weaksubj\":\n",
        "                    weight = 1\n",
        "                else: weight = weight_argument\n",
        "                    \n",
        "                if polarity == \"positive\":\n",
        "                    sentiment_score += 1 * weight\n",
        "                elif polarity == \"negative\":\n",
        "                    sentiment_score -= 1 * weight\n",
        "    if sentiment_score > 8:\n",
        "        result = \"POS\"\n",
        "    #print(\"Sentiment score: {}\".format(sentiment_score))\n",
        "    return result\n",
        "\n",
        "def get_weighted_results(lookup_table,reviews,weight):\n",
        "    targets = []\n",
        "    predictions = []\n",
        "    correctness_predictions = []\n",
        "    for review in reviews: \n",
        "        prediction = predict_review_weighted(lookup_table,review[\"content\"],weight)\n",
        "        predictions.append(prediction)\n",
        "        target = review[\"sentiment\"]\n",
        "        targets.append(target)\n",
        "        if prediction == target:\n",
        "            correctness_predictions.append(\"+\")\n",
        "        else: correctness_predictions.append(\"-\")\n",
        "            \n",
        "    return (predictions,targets,correctness_predictions)\n",
        "\n",
        "def run_binary_predictions(sent_lexicon,document_set):\n",
        "    lookup_table = create_lookup_extension(sent_lexicon,False)\n",
        "    binary_predictions,targets,correctness_results = get_binary_results(lookup_table,document_set)\n",
        "    binary_accuracy = calc_accuracy(binary_predictions,targets)\n",
        "    return (binary_predictions,binary_accuracy,correctness_results)\n",
        "\n",
        "def run_weighted_predictions(sent_lexicon,document_set,weight):\n",
        "    lookup_table = create_lookup_extension(sent_lexicon,True)\n",
        "    weighted_predictions,targets,correctness_results = get_weighted_results(lookup_table,document_set,weight)\n",
        "    weighted_accuracy = calc_accuracy(weighted_predictions,targets)\n",
        "    return (weighted_predictions,weighted_accuracy,correctness_results)\n",
        "\n",
        "def predict_binary_or_weighted(sent_lexicon=slex,document_set=reviews,prediction_type=\"B\",given_weight=4):\n",
        "    switcher = {\n",
        "        \"B\":run_binary_predictions(sent_lexicon,document_set),\n",
        "        \"W\":run_weighted_predictions(sent_lexicon,document_set,given_weight)\n",
        "    }\n",
        "    return switcher.get(prediction_type, ([],0)) #Returns predictions and accuracy\n",
        "    \n",
        "weighted_predictions, weighted_accuracy, weighted_correctness = predict_binary_or_weighted(slex,reviews,\"W\",4)\n",
        "binary_predictions, binary_accuracy,binary_correctness = predict_binary_or_weighted(slex,reviews,\"B\")\n",
        "print(\"Weighted Accuracy: {}%\".format(weighted_accuracy))\n",
        "print(\"Binary Accuracy: {}%\".format(binary_accuracy))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weighted Accuracy: 68.45%\n",
            "Binary Accuracy: 67.95%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9vVk7CvDpyka",
        "colab_type": "code",
        "outputId": "a4f6781a-5e5a-4f23-a0c4-23e5cc83f4b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "magnitude_results = weighted_correctness  # Correctness of sentiment predictons against the actual targets\n",
        "magnitude_accuracy = weighted_accuracy# 68.45%\n",
        "print(\"Accuracy: %0.2f\" % magnitude_accuracy)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 68.45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h9SHoGPfsAHV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Optional: make a barplot of the two results."
      ]
    },
    {
      "metadata": {
        "id": "8LgBcYcXsEk3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6MFrz8Jink0D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Answering questions in statistically significant ways (1pt)\n",
        "-------------------------------------------------------------"
      ]
    },
    {
      "metadata": {
        "id": "kxkxrldT9Ymc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Does using the magnitude improve the results? Oftentimes, answering questions like this about the performance of\n",
        "different signals and/or algorithms by simply looking at the output\n",
        "numbers is not enough. When dealing with natural language or human\n",
        "ratings, it’s safe to assume that there are infinitely many possible\n",
        "instances that could be used for training and testing, of which the ones\n",
        "we actually train and test on are a tiny sample. Thus, it is possible\n",
        "that observed differences in the reported performance are really just\n",
        "noise. \n",
        "\n",
        "There exist statistical methods which can be used to check for\n",
        "consistency (*statistical significance*) in the results, and one of the\n",
        "simplest such tests is the **sign test**. \n",
        "\n",
        "The sign test is based on the binomial distribution. Count all cases when System 1 is better than System 2, when System 2 is better than System 1, and when they are the same. Call these numbers $Plus$, $Minus$ and $Null$ respectively. \n",
        "\n",
        "The sign test returns the probability that the null hypothesis is true. \n",
        "\n",
        "This probability is called the $p$-value and it can be calculated for the two-sided sign test using the following formula (we multiply by two because this is a two-sided sign test and tests for the significance of differences in either direction):\n",
        "\n",
        "$$2 \\, \\sum\\limits_{i=0}^{k} \\binom{N}{i} \\, q^i \\, (1-q)^{N-i}$$\n",
        "\n",
        "where $$N = 2 \\Big\\lceil \\frac{Null}{2}\\Big\\rceil + Plus + Minus$$ is the total\n",
        "number of cases, and\n",
        "$$k = \\Big\\lceil \\frac{Null}{2}\\Big\\rceil + \\min\\{Plus,Minus\\}$$ is the number of\n",
        "cases with the less common sign. \n",
        "\n",
        "In this experiment, $q = 0.5$. Here, we\n",
        "treat ties by adding half a point to either side, rounding up to the\n",
        "nearest integer if necessary. \n",
        "\n",
        "\n",
        "#### (Q 2.1): Implement the sign test. Is the difference between the two symbolic systems significant? What is the p-value? (1 pt)\n",
        "\n",
        "You should use the `comb` function from `scipy` and the `decimal` package for the stable adding of numbers in the final summation.\n",
        "\n",
        "You can quickly verify the correctness of\n",
        "your sign test code using a [free online\n",
        "tool](https://www.graphpad.com/quickcalcs/binomial1.cfm)."
      ]
    },
    {
      "metadata": {
        "id": "de5l4oPkE-BS",
        "colab_type": "code",
        "outputId": "2a1e155c-52b3-4dbf-ab17-5ee0c8fcc7a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "from decimal import Decimal\n",
        "from scipy.special import comb\n",
        "\n",
        "\n",
        "def sign_test(results_1, results_2):\n",
        "  \"\"\"test for significance\n",
        "  results_1 is a list of classification results (+ for correct, - incorrect)\n",
        "  results_2 is a list of classification results (+ for correct, - incorrect)\n",
        "  \"\"\"\n",
        "  ties, plus, minus = 0, 0, 0\n",
        "\n",
        "  # \"-\" carries the error\n",
        "  for i in range(0, len(results_1)):\n",
        "    if results_1[i]==results_2[i]:\n",
        "      ties += 1\n",
        "    elif results_1[i]==\"-\": \n",
        "      plus += 1\n",
        "    elif results_2[i]==\"-\": \n",
        "      minus += 1\n",
        "\n",
        "  n = 2 * (ties/2) + plus + minus\n",
        "  k = (ties/2) + min([plus,minus])\n",
        "\n",
        "  summation = Decimal(0.0)\n",
        "  for i in range(0,int(k)+1):\n",
        "      summation += Decimal(comb(n, i,True)) # YOUR CODE HERE\n",
        "\n",
        "  # use two-tailed version of test\n",
        "  summation *= 2\n",
        "  summation *= (Decimal(0.5)**Decimal(n))\n",
        "  \n",
        "  print(\"the difference is\", \n",
        "        \"not significant\" if summation >= 0.05 else \"significant\")\n",
        "  \n",
        "  return summation\n",
        "\n",
        "p_value = sign_test(token_results, magnitude_results)\n",
        "print(\"p_value =\", p_value)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the difference is not significant\n",
            "p_value = 0.8405124276655436267484850072\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uhU_tk-BOaXb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using the Sign test\n",
        "\n",
        "**From now on, report all differences between systems using the\n",
        "sign test.** You can think about a change that you apply to one system, as a\n",
        " new system.\n",
        "    \n",
        "You should report statistical test\n",
        "results in an appropriate form – if there are several different methods\n",
        "(i.e., systems) to compare, tests can only be applied to pairs of them\n",
        "at a time. This creates a triangular matrix of test results in the\n",
        "general case. When reporting these pair-wise differences, you should\n",
        "summarise trends to avoid redundancy.\n"
      ]
    },
    {
      "metadata": {
        "id": "LibV4nR89BXb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Naive Bayes (8pt + 1pt bonus)\n",
        "=========="
      ]
    },
    {
      "metadata": {
        "id": "fnF9adQnuwia",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Your second task is to program a simple Machine Learning approach that operates\n",
        "on a simple Bag-of-Words (BoW) representation of the text data, as\n",
        "described in Pang et al. (2002). In this approach, the only features we\n",
        "will consider are the words in the text themselves, without bringing in\n",
        "external sources of information. The BoW model is a popular way of\n",
        "representing text information as vectors (or points in space), making it\n",
        "easy to apply classical Machine Learning algorithms on NLP tasks.\n",
        "However, the BoW representation is also very crude, since it discards\n",
        "all information related to word order and grammatical structure in the\n",
        "original text.\n",
        "\n",
        "## Writing your own classifier\n",
        "\n",
        "Write your own code to implement the Naive Bayes (NB) classifier. As\n",
        "a reminder, the Naive Bayes classifier works according to the following\n",
        "equation:\n",
        "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} P(c|\\bar{f}) = \\operatorname*{arg\\,max}_{c \\in C} P(c)\\prod^n_{i=1} P(f_i|c)$$\n",
        "where $C = \\{ \\text{POS}, \\text{NEG} \\}$ is the set of possible classes,\n",
        "$\\hat{c} \\in C$ is the most probable class, and $\\bar{f}$ is the feature\n",
        "vector. Remember that we use the log of these probabilities when making\n",
        "a prediction:\n",
        "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} \\Big\\{\\log P(c) + \\sum^n_{i=1} \\log P(f_i|c)\\Big\\}$$\n",
        "\n",
        "You can find more details about Naive Bayes in [Jurafsky &\n",
        "Martin](https://web.stanford.edu/~jurafsky/slp3/). You can also look at\n",
        "this helpful\n",
        "[pseudo-code](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html).\n",
        "\n",
        "*Note: this section and the next aim to put you a position to replicate\n",
        "    Pang et al., Naive Bayes results. However, the numerical results\n",
        "    will differ from theirs, as they used different data.*\n",
        "\n",
        "**You must write the Naive Bayes training and prediction code from\n",
        "scratch.** You will not be given credit for using off-the-shelf Machine\n",
        "Learning libraries.\n",
        "\n",
        "The data contains the text of the reviews, where each document consists\n",
        "of the sentences in the review, the sentiment of the review and an index\n",
        "(cv) that you will later use for cross-validation. You will find the\n",
        "text has already been tokenised and POS-tagged for you. Your algorithm\n",
        "should read in the text, **lowercase it**, and store the words and their\n",
        "frequencies in an appropriate data structure that allows for easy\n",
        "computation of the probabilities used in the Naive Bayes algorithm, and\n",
        "then make predictions for new instances."
      ]
    },
    {
      "metadata": {
        "id": "gsZRhaI3WvzC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### (Q3.1) Train your classifier on (positive and negative) reviews with cv-value 000-899, and test it on the remaining reviews cv900–cv999.  Report results using simple classification accuracy as your evaluation metric. Your  features are the word vocabulary. The value of a feature is the count of that feature (word) in the document. (2pts)\n"
      ]
    },
    {
      "metadata": {
        "id": "G7zaJYGFvIJ3",
        "colab_type": "code",
        "outputId": "184f8fd3-1b5f-47a5-d040-d26dbd56c3fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "#Current duration 66 seconds, see if it can be reduced\n",
        "\n",
        "from math import log\n",
        "from random import randint\n",
        "\n",
        "class NB_classifier():\n",
        " \n",
        "  def fit(self,classes,documents):\n",
        "    self.amount_docs = len(documents)\n",
        "    self.amount_doc_per_class = [0 for i in range(len(classes))] #Amount POS (class) documents is index 0 and NEG is index 1\n",
        "    self.prior_of_classes = [0.0 for i in range(len(classes))] \n",
        "    self.classes = classes\n",
        "    self.total_vocabulary = set()\n",
        "    self.class_word_counters = [Counter(),Counter()]\n",
        "    self.total_amount_words_class = [0 for i in range(len(classes))]\n",
        "    print(classes)\n",
        "    \n",
        "    for review in documents:\n",
        "      sentiment = review[\"sentiment\"]\n",
        "      content = review[\"content\"]\n",
        "      if sentiment == classes[0]: #Positive class\n",
        "        self.amount_doc_per_class[0] +=1\n",
        "        class_index = 0\n",
        "      else: #Negative class\n",
        "        self.amount_doc_per_class[1] +=1\n",
        "        class_index = 1\n",
        "\n",
        "      for sentence in content:\n",
        "        for token,pos_tag in sentence:\n",
        "          token = token.lower()\n",
        "          if token not in self.total_vocabulary:\n",
        "            self.total_vocabulary.add(token) \n",
        "           \n",
        "          counter_dict = self.class_word_counters[class_index]\n",
        "          counter_dict[token]  += 1\n",
        "          self.total_amount_words_class[class_index] += 1\n",
        "    \n",
        "    self.prior_of_classes[0] = self.amount_doc_per_class[0] / self.amount_docs\n",
        "    self.prior_of_classes[1] = self.amount_doc_per_class[1] / self.amount_docs\n",
        "    #print(\" Total amount pos in dict {}\".format(self.total_amount_words_class[0]))\n",
        "    #print(\" Total amount neg in dict {}\".format(self.total_amount_words_class[1]))\n",
        "   \n",
        "    \n",
        "  def predict(self,test_documents):\n",
        "      pos_prior = log(self.prior_of_classes[0]) if self.prior_of_classes[0] > 0 else 0\n",
        "      neg_prior = log(self.prior_of_classes[1]) if self.prior_of_classes[1] > 0 else 0\n",
        "      predictions = []\n",
        "      targets = []\n",
        "      pos_dict = self.class_word_counters[0]\n",
        "      neg_dict = self.class_word_counters[1]\n",
        "      \n",
        "      for review in test_documents:\n",
        "        targets.append(review[\"sentiment\"])\n",
        "        content = review[\"content\"]\n",
        "        probability_POS = pos_prior\n",
        "        probability_NEG = neg_prior\n",
        "        total_conditional_pos = 0\n",
        "        total_conditional_neg = 0 \n",
        "        \n",
        "        for sentence in content:\n",
        "          for token,pos_tag in sentence:\n",
        "            token = token.lower()\n",
        "            if token in self.total_vocabulary:\n",
        "              occ_in_pos =  pos_dict.get(token)\n",
        "              occ_in_neg = neg_dict.get(token)\n",
        "\n",
        "              if occ_in_pos != None:\n",
        "                #print(\"Word {} and occ {}\".format(token,occ_in_pos))\n",
        "                conditional_pos = occ_in_pos / self.total_amount_words_class[0]\n",
        "                total_conditional_pos += log(conditional_pos)\n",
        "              if occ_in_neg != None:\n",
        "                #print(\"Word {} and occ {}\".format(token,occ_in_neg))\n",
        "                conditional_neg = occ_in_neg / self.total_amount_words_class[1]\n",
        "                total_conditional_neg += log(conditional_neg)\n",
        "                \n",
        "        probability_POS += total_conditional_pos\n",
        "        probability_NEG += total_conditional_neg\n",
        "        probs = [probability_POS,probability_NEG]\n",
        "        max_prob = max(probs)\n",
        "        count = probs.count(max_prob)\n",
        "        if count == 1:\n",
        "          max_index = probs.index(max_prob)\n",
        "        else: max_index = randint(0, 1)\n",
        "        predictions.append(self.classes[max_index])\n",
        "        \n",
        "      return (predictions,targets)\n",
        "    \n",
        "  def get_correctness_results(self,predictions,targets):\n",
        "    correctness_predictions = []\n",
        "    for i,prediction in enumerate(predictions):\n",
        "      target = targets[i]\n",
        "      if (prediction == target):\n",
        "        correctness_predictions.append(\"+\")\n",
        "      else:\n",
        "        correctness_predictions.append(\"-\")\n",
        "    return correctness_predictions\n",
        "  \n",
        "\n",
        "classes = [\"POS\",\"NEG\"]\n",
        "neg_reviews =  reviews[0:1000]\n",
        "pos_reviews = reviews[1000:]\n",
        "train_set = neg_reviews[:900] + pos_reviews[:900]\n",
        "test_set = neg_reviews[900:] + pos_reviews[900:]\n",
        "print(len(train_set))\n",
        "print(len(test_set))\n",
        "\n",
        "nb = NB_classifier()\n",
        "nb.fit(classes,train_set)\n",
        "predictions,targets = nb.predict(test_set)\n",
        "acc = calc_accuracy(predictions,targets)\n",
        "no_smoothing_results = nb.get_correctness_results(predictions,targets)\n",
        "print(predictions)\n",
        "print(targets)\n",
        "print(\"Accuracy of no smoothing: {}%\".format(acc))"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1800\n",
            "200\n",
            "['POS', 'NEG']\n",
            "['POS', 'NEG', 'NEG', 'POS', 'POS', 'NEG', 'NEG', 'NEG', 'NEG', 'POS', 'POS', 'POS', 'NEG', 'POS', 'POS', 'NEG', 'NEG', 'POS', 'POS', 'NEG', 'POS', 'NEG', 'POS', 'POS', 'POS', 'POS', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'POS', 'NEG', 'POS', 'NEG', 'POS', 'POS', 'NEG', 'POS', 'NEG', 'NEG', 'POS', 'POS', 'POS', 'NEG', 'NEG', 'NEG', 'NEG', 'POS', 'POS', 'NEG', 'POS', 'POS', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'POS', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'POS', 'NEG', 'POS', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'POS', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'POS', 'POS', 'POS', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'POS', 'POS', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'POS', 'NEG', 'POS', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'POS', 'POS', 'POS', 'POS', 'NEG', 'NEG', 'POS', 'NEG', 'POS', 'NEG', 'NEG', 'POS', 'NEG', 'POS', 'POS', 'NEG', 'POS', 'POS', 'NEG', 'NEG', 'POS', 'POS', 'POS', 'NEG', 'NEG', 'NEG', 'POS', 'POS', 'NEG', 'POS', 'NEG', 'POS', 'POS', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'POS', 'POS', 'NEG', 'POS', 'NEG', 'POS', 'POS', 'POS', 'NEG']\n",
            "['NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS']\n",
            "Accuracy of no smoothing: 49.5%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0INK-PBoM6CB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### (Bonus Questions) Would you consider accuracy to also be a good way to evaluate your classifier in a situation where 90% of your data instances are of positive movie reviews? (1pt)\n",
        "\n",
        "You can simulate this scenario by keeping the positive reviews\n",
        "data unchanged, but only using negative reviews cv000–cv089 for\n",
        "training, and cv900–cv909 for testing. Calculate the classification\n",
        "accuracy, and explain what changed."
      ]
    },
    {
      "metadata": {
        "id": "GWDkt5ZrrFGp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "#Same as above, but rreplace train and test set with:\n",
        "train_set = neg_reviews[:90] + pos_reviews[:900]\n",
        "test_set = neg_reviews[900:910] + pos_reviews[900:]\n",
        "#Accuracy would not be a good measure, since there is dominance of a class in training and test set (class inbalance). \n",
        "#Therefore a classifier could just predict one class all the time and still achieve an high accuracy if that class is dominating in the test set.\n",
        "#It then would not really be a classifier anymore if just predicts one class the whole time, and is not able to distinguish between them\n",
        "#Thus, other measures that would be helpfull are precision and recall that also take False positive or false negatives into account. In this case, recall is probably more desired as positive reviews are dominating in test set."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6wJzcHX3WUDm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Smoothing\n",
        "\n",
        "The presence of words in the test dataset that\n",
        "haven’t been seen during training can cause probabilities in the Naive\n",
        "Bayes classifier to be $0$, thus making that particular test instance\n",
        "undecidable. The standard way to mitigate this effect (as well as to\n",
        "give more clout to rare words) is to use smoothing, in which the\n",
        "probability fraction\n",
        "$$\\frac{\\text{count}(w_i, c)}{\\sum\\limits_{w\\in V} \\text{count}(w, c)}$$ for a word\n",
        "$w_i$ becomes\n",
        "$$\\frac{\\text{count}(w_i, c) + \\text{smoothing}(w_i)}{\\sum\\limits_{w\\in V} \\text{count}(w, c) + \\sum\\limits_{w \\in V} \\text{smoothing}(w)}$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "PBNIcbwUWphC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### (Q3.2) Implement Laplace feature smoothing (1pt)\n",
        "($smoothing(\\cdot) = \\kappa$, constant for all words) in your Naive\n",
        "Bayes classifier’s code, and report the impact on performance. \n",
        "Use $\\kappa = 1$."
      ]
    },
    {
      "metadata": {
        "id": "g03yflCc9kpW",
        "colab_type": "code",
        "outputId": "b0902597-7870-4f9c-b562-a485bc03161b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE Same, as before, but with smoothing function\n",
        "\n",
        "from math import log\n",
        "from random import randint\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "class NB_classifier():\n",
        "  \n",
        "  def __init__(self,classes,smoothing_value=0,tostem=False):\n",
        "    self.smoothing_value = smoothing_value\n",
        "    self.tostem = tostem\n",
        "    self.classes = classes\n",
        "\n",
        "  def fit(self,documents):\n",
        "    self.amount_docs = len(documents)\n",
        "    self.amount_doc_per_class = [0] * len(classes) #Amount POS (class) documents is index 0 and NEG is index 1\n",
        "    self.prior_of_classes = [0.0] * len(classes) \n",
        "    self.total_vocabulary = set()\n",
        "    self.class_word_counters = [Counter(),Counter()]\n",
        "    self.total_amount_words_class = [0] * len(classes)\n",
        "\n",
        "    for review in documents:\n",
        "      sentiment = review[\"sentiment\"]\n",
        "      content = review[\"content\"]\n",
        "      \n",
        "      if sentiment == classes[0]: #Positive class\n",
        "        self.amount_doc_per_class[0] +=1\n",
        "        class_index = 0\n",
        "      else: #Negative class\n",
        "        self.amount_doc_per_class[1] +=1\n",
        "        class_index = 1\n",
        "\n",
        "      for sentence in content:\n",
        "        for token,pos_tag in sentence:\n",
        "          token = token.lower()\n",
        "          if self.tostem:\n",
        "             token = stemmer.stem(token)\n",
        "          if token not in self.total_vocabulary:\n",
        "            self.total_vocabulary.add(token) \n",
        "           \n",
        "          counter_dict = self.class_word_counters[class_index]\n",
        "          counter_dict[token]  += 1\n",
        "          self.total_amount_words_class[class_index] += 1\n",
        "    \n",
        "    self.prior_of_classes[0] = self.amount_doc_per_class[0] / self.amount_docs\n",
        "    self.prior_of_classes[1] = self.amount_doc_per_class[1] / self.amount_docs\n",
        "    #print(\" Total amount pos in dict {}\".format(self.total_amount_words_class[0]))\n",
        "    #print(\" Total amount neg in dict {}\".format(self.total_amount_words_class[1]))\n",
        "   \n",
        "    \n",
        "  def predict(self,test_documents):\n",
        "      pos_prior = log(self.prior_of_classes[0]) if self.prior_of_classes[0] > 0 else 0\n",
        "      neg_prior = log(self.prior_of_classes[1]) if self.prior_of_classes[1] > 0 else 0\n",
        "      predictions = []\n",
        "      pos_dict = self.class_word_counters[0]\n",
        "      neg_dict = self.class_word_counters[1]\n",
        "      \n",
        "      for review in test_documents:\n",
        "        content = review[\"content\"]\n",
        "        probability_POS = pos_prior\n",
        "        probability_NEG = neg_prior\n",
        "        total_conditional_pos = 0\n",
        "        total_conditional_neg = 0 \n",
        "        \n",
        "        for sentence in content:\n",
        "          for token,pos_tag in sentence:\n",
        "            token = token.lower()\n",
        "            if self.tostem:\n",
        "                token = stemmer.stem(token)\n",
        "            if token in self.total_vocabulary:\n",
        "              occ_in_pos = pos_dict[token] if token in self.class_word_counters[0] else 0\n",
        "              occ_in_neg = neg_dict[token] if token in self.class_word_counters[1] else 0\n",
        "\n",
        "              #Added change for smoothing, occ_in_pos and occ_in_neg = 0 initial and add smoothing by conditional\n",
        "              if self.smoothing_value > 0:\n",
        "                  conditional_pos = (occ_in_pos + self.smoothing_value) / (self.total_amount_words_class[0] + len(self.total_vocabulary) * self.smoothing_value)\n",
        "                  conditional_neg = (occ_in_neg + self.smoothing_value) / (self.total_amount_words_class[1] + len(self.total_vocabulary) * self.smoothing_value)\n",
        "              else: \n",
        "                  conditional_pos = occ_in_pos / self.total_amount_words_class[0]\n",
        "                  conditional_neg = occ_in_neg / self.total_amount_words_class[1]\n",
        "              total_conditional_pos += log(conditional_pos)\n",
        "              total_conditional_neg += log(conditional_neg)\n",
        "                \n",
        "        probability_POS += total_conditional_pos\n",
        "        probability_NEG += total_conditional_neg\n",
        "        probs = [probability_POS,probability_NEG]\n",
        "        max_prob = max(probs)\n",
        "        count = probs.count(max_prob)\n",
        "        if count == 1:\n",
        "          max_index = probs.index(max_prob)\n",
        "        else: max_index = randint(0, 1)\n",
        "        predictions.append(self.classes[max_index])\n",
        "        \n",
        "      return predictions\n",
        "    \n",
        "def get_correctness_results(predictions,targets):\n",
        "    correctness_predictions = []\n",
        "      \n",
        "    for i,prediction in enumerate(predictions):\n",
        "      target = targets[i]\n",
        "      if (prediction == target):\n",
        "         correctness_predictions.append(\"+\")\n",
        "      else:\n",
        "         correctness_predictions.append(\"-\")\n",
        "        \n",
        "    return correctness_predictions\n",
        "    \n",
        "\n",
        "classes = [\"POS\",\"NEG\"]\n",
        "neg_reviews =  reviews[0:1000]\n",
        "pos_reviews = reviews[1000:]\n",
        "train_set = neg_reviews[:900] + pos_reviews[:900]\n",
        "test_set = neg_reviews[900:] + pos_reviews[900:]\n",
        "print(len(train_set))\n",
        "print(len(test_set))\n",
        "\n",
        "nb = NB_classifier(classes,1)\n",
        "nb.fit(train_set)\n",
        "smoothing_predictions = nb.predict(test_set)\n",
        "targets = []\n",
        "for review in test_set:\n",
        "    targets.append(review[\"sentiment\"])\n",
        "\n",
        "acc = calc_accuracy(smoothing_predictions,targets)\n",
        "smoothing_results = get_correctness_results(smoothing_predictions,targets)\n",
        "print(predictions)\n",
        "print(targets)\n",
        "print(\"Accuracy of smoothing: {}%\".format(acc))\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1800\n",
            "200\n",
            "['POS', 'NEG', 'NEG', 'POS', 'POS', 'NEG', 'NEG', 'NEG', 'NEG', 'POS', 'POS', 'POS', 'NEG', 'POS', 'POS', 'NEG', 'NEG', 'POS', 'POS', 'NEG', 'POS', 'NEG', 'POS', 'POS', 'POS', 'POS', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'POS', 'NEG', 'POS', 'NEG', 'POS', 'POS', 'NEG', 'POS', 'NEG', 'NEG', 'POS', 'POS', 'POS', 'NEG', 'NEG', 'NEG', 'NEG', 'POS', 'POS', 'NEG', 'POS', 'POS', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'POS', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'POS', 'NEG', 'POS', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'POS', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'POS', 'POS', 'POS', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'POS', 'POS', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'POS', 'NEG', 'POS', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'POS', 'POS', 'POS', 'POS', 'NEG', 'NEG', 'POS', 'NEG', 'POS', 'NEG', 'NEG', 'POS', 'NEG', 'POS', 'POS', 'NEG', 'POS', 'POS', 'NEG', 'NEG', 'POS', 'POS', 'POS', 'NEG', 'NEG', 'NEG', 'POS', 'POS', 'NEG', 'POS', 'NEG', 'POS', 'POS', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'POS', 'POS', 'NEG', 'POS', 'NEG', 'POS', 'POS', 'POS', 'NEG']\n",
            "['NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'NEG', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS', 'POS']\n",
            "Accuracy of smoothing: 82.5%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-conSBddWWyN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### (Q3.3) Is the difference between non smoothed (Q3.1) and smoothed (Q3.2) statistically significant? (0.5pt)"
      ]
    },
    {
      "metadata": {
        "id": "CCvSNGlHMUPz",
        "colab_type": "code",
        "outputId": "845750ab-33e1-4fa7-a069-2100cf21f147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Different is statiscally significant\n",
        "p_value = sign_test(no_smoothing_results, smoothing_results)\n",
        "print(\"p_value =\", p_value)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the difference is significant\n",
            "p_value = 0.000003547178174130642586494974890\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZiGcgwba87D5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Cross-validation\n",
        "\n",
        "A serious danger in using Machine Learning on small datasets, with many\n",
        "iterations of slightly different versions of the algorithms, is that we\n",
        "end up with Type III errors, also called the “testing hypotheses\n",
        "suggested by the data” errors. This type of error occurs when we make\n",
        "repeated improvements to our classifiers by playing with features and\n",
        "their processing, but we don’t get a fresh, never-before seen test\n",
        "dataset every time. Thus, we risk developing a classifier that’s better\n",
        "and better on our data, but worse and worse at generalizing to new,\n",
        "never-before seen data.\n",
        "\n",
        "A simple method to guard against Type III errors is to use\n",
        "cross-validation. In N-fold cross-validation, we divide the data into N\n",
        "distinct chunks / folds. Then, we repeat the experiment N times, each\n",
        "time holding out one of the chunks for testing, training our classifier\n",
        "on the remaining N - 1 data chunks, and reporting performance on the\n",
        "held-out chunk. We can use different strategies for dividing the data:\n",
        "\n",
        "-   Consecutive splitting:\n",
        "  - cv000–cv099 = Split 1\n",
        "  - cv100–cv199 = Split 2\n",
        "  - etc.\n",
        "  \n",
        "-   Round-robin splitting (mod 10):\n",
        "  - cv000, cv010, cv020, … = Split 1\n",
        "  - cv001, cv011, cv021, … = Split 2\n",
        "  - etc.\n",
        "\n",
        "-   Random sampling/splitting\n",
        "  - Not used here (but you may choose to split this way in a non-educational situation)\n",
        "\n",
        "#### (Q3.4) Write the code to implement 10-fold cross-validation using round-robin splitting for your Naive Bayes classifier from Q3.2 and compute the 10 accuracies. Report the final performance, which is the average of the performances per fold. If all splits perform equally well, this is a good sign. (1pt)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3KeCGPa7Nuzx",
        "colab_type": "code",
        "outputId": "79c5d6b4-99ce-4345-9379-f3e04e4bf8e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "#Takes a while\n",
        "\n",
        "#Create folds according to the round-robin (mod 10) as that is the amount of folds\n",
        "def create_folds(data,amount_folds):\n",
        "  all_folds = [[] for i in range(amount_folds)]\n",
        "  review_index = 0\n",
        "  while review_index < len(data):\n",
        "    for i,fold in enumerate(all_folds):\n",
        "        review = data[review_index + i]\n",
        "        fold.append(review)\n",
        "    review_index += amount_folds\n",
        "   \n",
        "  return all_folds\n",
        "\n",
        "def cross_validation(classifier,data,amount_folds,preprocessing=False,include_pos_tag=False,vocabulary=None,preference_pos=False):\n",
        "    all_folds = create_folds(reviews,10)\n",
        "    folds_performance = []\n",
        "    correctness_results = []\n",
        "    \n",
        "    for i,test_fold in enumerate(all_folds):\n",
        "      training_set = []\n",
        "      for j in range(10):\n",
        "        if i != j:\n",
        "          training_fold = all_folds[j]\n",
        "          training_set += training_fold\n",
        "         \n",
        "      if preprocessing: #Preprocessing training and test data for SVM\n",
        "        features,train_targets,voc = get_FeaturesAndTargets(training_set,include_pos_tag,vocabulary,preference_pos)\n",
        "        x_train = features\n",
        "        y_train = train_targets\n",
        "        classifier.fit(x_train,y_train)\n",
        "\n",
        "        x_test,y_test,voc = get_FeaturesAndTargets(test_fold,include_pos_tag,voc,preference_pos)\n",
        "        targets = y_test\n",
        "        predictions = classifier.predict(x_test)\n",
        "        \n",
        "      else:\n",
        "        targets = []\n",
        "        for review in test_fold:\n",
        "           targets.append(review[\"sentiment\"])\n",
        "\n",
        "        classifier.fit(training_set) #Classifier training resets\n",
        "        predictions = classifier.predict(test_fold)\n",
        "        \n",
        "      acc = calc_accuracy(predictions,targets)\n",
        "      print(\"Accuracy of fold {}: {}\".format(i+1,acc))\n",
        "      folds_performance.append(acc)\n",
        "      results = get_correctness_results(predictions,targets)\n",
        "      correctness_results += results\n",
        "    \n",
        "    sum_acc = sum(folds_performance)\n",
        "    mean_acc = sum_acc/amount_folds\n",
        "    print(\"Final performance average: {}\".format(mean_acc))\n",
        "    \n",
        "    return (folds_performance,mean_acc,correctness_results)\n",
        "  \n",
        "nb = NB_classifier(classes,1)\n",
        "folds_performance, mean_acc,smoothing_cv_results = cross_validation(nb,reviews,10)\n",
        "\n",
        "\n"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of fold 1: 79.0\n",
            "Accuracy of fold 2: 83.5\n",
            "Accuracy of fold 3: 80.5\n",
            "Accuracy of fold 4: 82.5\n",
            "Accuracy of fold 5: 78.0\n",
            "Accuracy of fold 6: 84.5\n",
            "Accuracy of fold 7: 83.0\n",
            "Accuracy of fold 8: 77.5\n",
            "Accuracy of fold 9: 83.0\n",
            "Accuracy of fold 10: 84.0\n",
            "Final performance average: 81.55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "otdlsDXBNyOa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### (Q3.5) Write code to calculate and report variance, in addition to the final performance. (1pt)\n",
        "\n",
        "**Please report all future results using 10-fold cross-validation now\n",
        "(unless told to use the held-out test set).**"
      ]
    },
    {
      "metadata": {
        "id": "ZoBQm1KuNzNR",
        "colab_type": "code",
        "outputId": "9d162edf-84b9-4bbb-8476-d6d4eacb3806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "print(folds_performance)\n",
        "array = np.array(folds_performance)\n",
        "var = np.var(array)\n",
        "print(\"Variance: {}\".format(var))\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[79.0, 83.5, 80.5, 82.5, 78.0, 84.5, 83.0, 77.5, 83.0, 84.0]\n",
            "Variance: 6.0225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s6A2zX9_BRKm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Features, overfitting, and the curse of dimensionality\n",
        "\n",
        "In the Bag-of-Words model, ideally we would like each distinct word in\n",
        "the text to be mapped to its own dimension in the output vector\n",
        "representation. However, real world text is messy, and we need to decide\n",
        "on what we consider to be a word. For example, is “`word`\" different\n",
        "from “`Word`\", from “`word`”, or from “`words`\"? Too strict a\n",
        "definition, and the number of features explodes, while our algorithm\n",
        "fails to learn anything generalisable. Too lax, and we risk destroying\n",
        "our learning signal. In the following section, you will learn about\n",
        "confronting the feature sparsity and the overfitting problems as they\n",
        "occur in NLP classification tasks."
      ]
    },
    {
      "metadata": {
        "id": "EKK8FNt8VtcZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### (Q3.6): A touch of linguistics (1pt)\n",
        "\n",
        "Taking a step further, you can use stemming to\n",
        "hash different inflections of a word to the same feature in the BoW\n",
        "vector space. How does the performance of your classifier change when\n",
        "you use stemming on your training and test datasets? Please use the [Porter stemming\n",
        "    algorithm](http://www.nltk.org/howto/stem.html) from NLTK.\n",
        " Also, you should do cross validation and concatenate the predictions from all folds to compute the significance."
      ]
    },
    {
      "metadata": {
        "id": "NxtCul1IrBi_",
        "colab_type": "code",
        "outputId": "dbe4d30e-f74d-4606-a7b7-3410c70f8a14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "nb = NB_classifier(classes,1,True)\n",
        "stem_folds_performance, stem_mean_acc, stem_cv_results = cross_validation(nb,reviews,10)\n",
        "#The performance does not change too much in terms of accuracy,but execution time is increased due to the overhead of stemming. \n",
        "\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of fold 1: 78.0\n",
            "Accuracy of fold 2: 84.0\n",
            "Accuracy of fold 3: 80.5\n",
            "Accuracy of fold 4: 84.0\n",
            "Accuracy of fold 5: 77.5\n",
            "Accuracy of fold 6: 84.0\n",
            "Accuracy of fold 7: 82.0\n",
            "Accuracy of fold 8: 77.5\n",
            "Accuracy of fold 9: 83.0\n",
            "Accuracy of fold 10: 83.0\n",
            "Final performance average: 81.35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6SrJ1BeLXTnk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### (Q3.7): Is the difference between NB with smoothing and NB with smoothing+stemming significant? (0.5pt)\n"
      ]
    },
    {
      "metadata": {
        "id": "gYqKBOiIrInT",
        "colab_type": "code",
        "outputId": "5f61afd0-c8c3-4744-c19e-c8af6bf41215",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# No it is not significant\n",
        "p_value = sign_test(smoothing_cv_results, stem_cv_results)\n",
        "print(\"p_value =\", p_value)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the difference is not significant\n",
            "p_value = 0.9465186089423488346539016590\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JkDHVq_1XUVP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Q3.8: What happens to the number of features (i.e., the size of the vocabulary) when using stemming as opposed to (Q3.2)? (0.5pt)\n",
        "Give actual numbers. You can use the held-out training set to determine these."
      ]
    },
    {
      "metadata": {
        "id": "MA3vee5-rJyy",
        "colab_type": "code",
        "outputId": "fa68e3a5-2716-4395-8834-7a360cbf05a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "classes = [\"POS\",\"NEG\"]\n",
        "neg_reviews =  reviews[0:1000]\n",
        "pos_reviews = reviews[1000:]\n",
        "train_set = neg_reviews[:900] + pos_reviews[:900]\n",
        "\n",
        "nb = NB_classifier(classes)\n",
        "nb.fit(train_set)\n",
        "print(\"Size of vocabulary without stemming: {}\".format( len(nb.total_vocabulary) ))\n",
        "\n",
        "nb = NB_classifier(classes,1,True)\n",
        "nb.fit(train_set)\n",
        "print(\"Size of vocabulary with stemming: {}\".format( len(nb.total_vocabulary) ))\n",
        "\n",
        "#The number of features decreases with stemming, which makes sense as many words can be simplified to the same canonical form. Therefore, 5 features can become 1 feature for example. "
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of vocabulary without stemming: 45348\n",
            "Size of vocabulary with stemming: 32404\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SoazfxbNV5Lq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Q3.9: Putting some word order back in (0.5+0.5pt=1pt)\n",
        "\n",
        "A simple way of retaining some of the word\n",
        "order information when using bag-of-words representations is to add **n-grams** features. \n",
        "Retrain your classifier from (Q3.4) using **unigrams+bigrams** and\n",
        "**unigrams+bigrams+trigrams** as features, and report accuracy and statistical significances (in comparison to the experiment at (Q3.4) for all 10 folds, and between the new systems).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "eYuKMTOpq9jz",
        "colab_type": "code",
        "outputId": "5d9523be-03d4-4381-e32f-78dcb14a1ec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "cell_type": "code",
      "source": [
        "# \n",
        "from math import log\n",
        "from random import randint\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "class NB_classifier():\n",
        "  \n",
        "  def __init__(self,classes,smoothing_value=0,tostem=False,uni_bi_or_trigram=1):\n",
        "    self.smoothing_value = smoothing_value\n",
        "    self.tostem = tostem\n",
        "    self.classes = classes\n",
        "    self.uni_bi_or_trigram = uni_bi_or_trigram\n",
        "\n",
        "  def fit(self,documents):\n",
        "    self.amount_docs = len(documents)\n",
        "    self.amount_doc_per_class = [0] * len(classes) #Amount POS (class) documents is index 0 and NEG is index 1\n",
        "    self.prior_of_classes = [0.0] * len(classes) \n",
        "    self.class_word_counters_unigram = [Counter(),Counter()]\n",
        "    self.class_word_counters_bigram = [{},{}]\n",
        "    self.class_word_counters_trigram = [{},{}]\n",
        "    self.total_amount_words_class = [0] * len(classes)\n",
        "    self.unigram_voc = set()\n",
        "    self.bigram_voc = set()\n",
        "    self.trigram_voc = set()\n",
        "    \n",
        "\n",
        "    for review in documents:\n",
        "      sentiment = review[\"sentiment\"]\n",
        "      content = review[\"content\"]\n",
        "      full_text = []\n",
        "      \n",
        "      if sentiment == classes[0]: #Positive class\n",
        "        self.amount_doc_per_class[0] +=1\n",
        "        class_index = 0\n",
        "      else: #Negative class\n",
        "        self.amount_doc_per_class[1] +=1\n",
        "        class_index = 1\n",
        "\n",
        "      for sentence in content:\n",
        "        for token,pos_tag in sentence:\n",
        "          token = token.lower()\n",
        "          if self.tostem:\n",
        "             token = stemmer.stem(token)\n",
        "          if token not in self.unigram_voc:\n",
        "            self.unigram_voc.add(token) \n",
        "          full_text.append(token)\n",
        "           \n",
        "          counter_dict_unigram = self.class_word_counters_unigram[class_index]\n",
        "          counter_dict_unigram[token]  += 1\n",
        "          self.total_amount_words_class[class_index] += 1\n",
        "          \n",
        "      #Bigram part\n",
        "      if self.uni_bi_or_trigram > 1:\n",
        "          bigrams = list(ngrams(full_text,2))\n",
        "          prior_word_dict = self.class_word_counters_bigram[class_index]\n",
        "          for bigram in bigrams:\n",
        "              if bigram not in self.bigram_voc:\n",
        "                  self.bigram_voc.add(bigram)\n",
        "            \n",
        "              first_word = bigram[0]\n",
        "              if first_word not in prior_word_dict:\n",
        "                bigram_with_prior_dict = {bigram: 1} #Creates a dictionary for a specific bigram combination with that prior word \n",
        "                prior_word_dict[first_word] = [1,bigram_with_prior_dict] #The first index stand for how many counts with prior word, second element is a dictioanry of bigrams with that prior\n",
        "              else: \n",
        "                prior_word_dict[first_word][0] += 1 #Increment count of all bigram with that prior\n",
        "                bigram_with_prior_dict = prior_word_dict[first_word][1] #Count of a specific bigram with that prior, to be incremented below\n",
        "                if bigram not in bigram_with_prior_dict:\n",
        "                  bigram_with_prior_dict[bigram] = 1\n",
        "                else: \n",
        "                  bigram_with_prior_dict[bigram] += 1\n",
        "      #Trigram part\n",
        "      if self.uni_bi_or_trigram == 3:\n",
        "          trigrams = list(ngrams(full_text,3))\n",
        "          prior_word_dict = self.class_word_counters_trigram[class_index]\n",
        "          for trigram in trigrams:\n",
        "              if trigram not in self.trigram_voc:\n",
        "                  self.trigram_voc.add(trigram)\n",
        "            \n",
        "              first_word = trigram[0]\n",
        "              second_word = trigram[1]\n",
        "              prior_word = first_word + second_word\n",
        "              if prior_word not in prior_word_dict:\n",
        "                trigram_with_prior_dict = {trigram: 1} #Creates a dictionary for a specific bigram combination with that prior word \n",
        "                prior_word_dict[prior_word] = [1,trigram_with_prior_dict] #The first index stand for how many counts with prior word, second element is a dictioanry of bigrams with that prior\n",
        "              else: \n",
        "                prior_word_dict[prior_word][0] += 1 #Increment count of all bigram with that prior\n",
        "                trigram_with_prior_dict = prior_word_dict[prior_word][1] #Count of a specific bigram with that prior, to be incremented below\n",
        "                if trigram not in trigram_with_prior_dict:\n",
        "                  trigram_with_prior_dict[trigram] = 1\n",
        "                else: \n",
        "                  trigram_with_prior_dict[trigram] += 1\n",
        "         \n",
        "        \n",
        "    self.prior_of_classes[0] = self.amount_doc_per_class[0] / self.amount_docs\n",
        "    self.prior_of_classes[1] = self.amount_doc_per_class[1] / self.amount_docs\n",
        "    self.total_vocabulary = self.unigram_voc|self.bigram_voc|self.trigram_voc #For unigram, bigram and trigram\n",
        "  \n",
        "  def predict(self,test_documents):\n",
        "      pos_prior = log(self.prior_of_classes[0]) if self.prior_of_classes[0] > 0 else 0\n",
        "      neg_prior = log(self.prior_of_classes[1]) if self.prior_of_classes[1] > 0 else 0\n",
        "      predictions = []\n",
        "      uni_pos_dict = self.class_word_counters_unigram[0]\n",
        "      uni_neg_dict = self.class_word_counters_unigram[1]\n",
        "      bi_pos_dict = self.class_word_counters_bigram[0]\n",
        "      bi_neg_dict = self.class_word_counters_bigram[1]\n",
        "      tri_pos_dict = self.class_word_counters_trigram[0]\n",
        "      tri_neg_dict = self.class_word_counters_trigram[1]\n",
        "      len_unigram_voc = len(self.unigram_voc)\n",
        "      len_bigram_voc = len(self.bigram_voc)\n",
        "      len_trigram_voc = len(self.trigram_voc)\n",
        "      weight = 1 / self.uni_bi_or_trigram\n",
        "      \n",
        "      for review in test_documents:\n",
        "        content = review[\"content\"]\n",
        "        probability_POS = pos_prior\n",
        "        probability_NEG = neg_prior\n",
        "        uni_total_conditional_pos = 0\n",
        "        uni_total_conditional_neg = 0 \n",
        "        bi_total_conditional_pos = 0\n",
        "        bi_total_conditional_neg = 0 \n",
        "        tri_total_conditional_pos = 0\n",
        "        tri_total_conditional_neg = 0 \n",
        "        full_text = []\n",
        "        \n",
        "        for sentence in content:\n",
        "          for token,pos_tag in sentence:\n",
        "            token = token.lower()\n",
        "            if self.tostem:\n",
        "                token = stemmer.stem(token)\n",
        "            if token in self.unigram_voc:\n",
        "              full_text.append(token)\n",
        "              occ_in_pos = uni_pos_dict[token] if token in uni_pos_dict else 0\n",
        "              occ_in_neg = uni_neg_dict[token] if token in uni_neg_dict  else 0\n",
        "\n",
        "              total_pos_words = self.total_amount_words_class[0]\n",
        "              total_neg_words = self.total_amount_words_class[1]\n",
        "              \n",
        "              uni_conditional_pos, uni_conditional_neg  = self.compute_conditional(occ_in_pos,total_pos_words,occ_in_neg,total_neg_words,self.smoothing_value,len_unigram_voc)\n",
        "              uni_total_conditional_pos += log(uni_conditional_pos)\n",
        "              uni_total_conditional_neg += log(uni_conditional_neg)\n",
        "                \n",
        "        probability_POS += weight * uni_total_conditional_pos\n",
        "        probability_NEG += weight * uni_total_conditional_neg\n",
        "        \n",
        "        #bigram part\n",
        "        if self.uni_bi_or_trigram > 1:\n",
        "          bigrams = list(ngrams(full_text,2))\n",
        "          for bigram in bigrams:\n",
        "              if bigram in self.bigram_voc:\n",
        "                first_word = bigram[0]\n",
        "                pos_prior_word_mapping = bi_pos_dict.get(first_word)\n",
        "                neg_prior_word_mapping = bi_neg_dict .get(first_word)\n",
        "                pos_bigram_count = 0\n",
        "                neg_bigram_count = 0\n",
        "                pos_prior_word_sequence_count = 1 #Assign value before reference in compute conditional, 0 dividing by 1 is still 0. \n",
        "                neg_prior_word_sequence_count = 1\n",
        " \n",
        "                if pos_prior_word_mapping != None:\n",
        "                    bigram_count = pos_prior_word_mapping[1].get(bigram)\n",
        "                    pos_bigram_count = bigram_count if bigram_count != None else 0\n",
        "                    pos_prior_word_sequence_count = pos_prior_word_mapping[0]\n",
        "                                                              \n",
        "                if neg_prior_word_mapping != None:\n",
        "                    bigram_count = neg_prior_word_mapping[1].get(bigram)\n",
        "                    neg_bigram_count = bigram_count if bigram_count != None else 0\n",
        "                    neg_prior_word_sequence_count = neg_prior_word_mapping[0]\n",
        "                  \n",
        "                bi_conditional_pos, bi_conditional_neg = self.compute_conditional(pos_bigram_count,pos_prior_word_sequence_count,neg_bigram_count,neg_prior_word_sequence_count,self.smoothing_value,len_bigram_voc)\n",
        "                bi_total_conditional_pos += log(bi_conditional_pos)\n",
        "                bi_total_conditional_neg += log(bi_conditional_neg)\n",
        "                \n",
        "          probability_POS += weight * bi_total_conditional_pos\n",
        "          probability_NEG += weight * bi_total_conditional_neg\n",
        "          \n",
        "        #trigram part \n",
        "        if self.uni_bi_or_trigram == 3:\n",
        "          trigrams = list(ngrams(full_text,3))\n",
        "          \n",
        "          for trigram in trigrams:\n",
        "              if trigram in self.trigram_voc:\n",
        "                prior_word = trigram[0] + trigram[1]\n",
        "                pos_prior_word_mapping = tri_pos_dict.get(prior_word)\n",
        "                neg_prior_word_mapping = tri_neg_dict.get(prior_word)\n",
        "                pos_trigram_count = 0\n",
        "                neg_trigram_count = 0\n",
        "                tri_pos_prior_word_sequence_count = 1\n",
        "                tri_neg_prior_word_sequence_count = 1\n",
        "   \n",
        "                if pos_prior_word_mapping != None:\n",
        "                    trigram_count = pos_prior_word_mapping[1].get(trigram)\n",
        "                    pos_trigram_count = trigram_count if trigram_count != None else 0\n",
        "                    tri_pos_prior_word_sequence_count = pos_prior_word_mapping[0]\n",
        "                                                              \n",
        "                if neg_prior_word_mapping != None:\n",
        "                    trigram_count = neg_prior_word_mapping[1].get(trigram)\n",
        "                    neg_trigram_count = trigram_count if trigram_count != None else 0\n",
        "                    tri_neg_prior_word_sequence_count = neg_prior_word_mapping[0]\n",
        "                  \n",
        "                tri_conditional_pos, tri_conditional_neg = self.compute_conditional(pos_trigram_count,tri_pos_prior_word_sequence_count,neg_trigram_count,tri_neg_prior_word_sequence_count,self.smoothing_value,len_trigram_voc)\n",
        "                \n",
        "                tri_total_conditional_pos += log(tri_conditional_pos)\n",
        "                tri_total_conditional_neg += log(tri_conditional_neg)\n",
        "          probability_POS += weight * tri_total_conditional_pos\n",
        "          probability_NEG += weight * tri_total_conditional_neg\n",
        "              \n",
        "        probs = [probability_POS,probability_NEG]\n",
        "        max_prob = max(probs)\n",
        "        count = probs.count(max_prob)\n",
        "        if count == 1:\n",
        "          max_index = probs.index(max_prob)\n",
        "        else: max_index = randint(0, 1)\n",
        "        predictions.append(self.classes[max_index])\n",
        "        \n",
        "      return predictions\n",
        "  \n",
        "  def compute_conditional(self,num_value_pos,denom_value_pos,num_value_neg,denom_value_neg,smoothing_value,len_voc):\n",
        "      conditional_pos=  0\n",
        "      conditional_pos = 0\n",
        "      \n",
        "      if smoothing_value > 0:\n",
        "          conditional_pos = (num_value_pos + smoothing_value) / (denom_value_pos + len_voc * smoothing_value)\n",
        "          conditional_neg = (num_value_neg + smoothing_value) / (denom_value_neg + len_voc * smoothing_value)\n",
        "      else:\n",
        "          conditional_pos = num_value_pos / denom_value_pos\n",
        "          conditional_neg = num_value_neg / denom_value_neg\n",
        "      \n",
        "      return (conditional_pos,conditional_neg)\n",
        "    \n",
        "  \n",
        "\n",
        "#All with smoothing and no stemming\n",
        "#3.4  \n",
        "\n",
        "print(\"Unigram only\")\n",
        "nb = NB_classifier(classes,1)\n",
        "uni_folds_performance, uni_mean_acc,uni_smoothing_cv_results = cross_validation(nb,reviews,10)\n",
        "\n",
        "#3.9 new added unigram+bigrams and unigram+bigrams+trigrams\n",
        "print(\"\\nUnigram + Bigram\")\n",
        "nb = NB_classifier(classes,1,False,2)\n",
        "bi_folds_performance, bi_mean_acc,bi_smoothing_cv_results = cross_validation(nb,reviews,10)\n",
        "\n",
        "print(\"\\nUnigram + Bigram + Trigram\")\n",
        "nb = NB_classifier(classes,1,False,3)\n",
        "tri_folds_performance, tri_mean_acc,tri_smoothing_cv_results = cross_validation(nb,reviews,10)\n",
        "\n",
        "\n",
        "print(\"\\nSign test between unigram and unigram+bigram: p_value =\", p_value)\n",
        "p_value = sign_test(uni_smoothing_cv_results, bi_smoothing_cv_results)\n",
        "print(\"\\nSign test between unigram and unigram+bigram+trigram: p_value =\", p_value)\n",
        "p_value = sign_test(uni_smoothing_cv_results, tri_smoothing_cv_results)\n",
        "print(\"\\nSign test between unigram+bigram and unigram+bigram+trigram: p_value =\", p_value)\n",
        "p_value = sign_test(bi_smoothing_cv_results, tri_smoothing_cv_results)\n",
        "\n"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unigram only\n",
            "Accuracy of fold 1: 79.0\n",
            "Accuracy of fold 2: 83.5\n",
            "Accuracy of fold 3: 80.5\n",
            "Accuracy of fold 4: 82.5\n",
            "Accuracy of fold 5: 78.0\n",
            "Accuracy of fold 6: 84.5\n",
            "Accuracy of fold 7: 83.0\n",
            "Accuracy of fold 8: 77.5\n",
            "Accuracy of fold 9: 83.0\n",
            "Accuracy of fold 10: 84.0\n",
            "Final performance average: 81.55\n",
            "\n",
            "Unigram + Bigram\n",
            "Accuracy of fold 1: 76.5\n",
            "Accuracy of fold 2: 79.0\n",
            "Accuracy of fold 3: 81.0\n",
            "Accuracy of fold 4: 84.0\n",
            "Accuracy of fold 5: 81.5\n",
            "Accuracy of fold 6: 83.0\n",
            "Accuracy of fold 7: 84.0\n",
            "Accuracy of fold 8: 79.0\n",
            "Accuracy of fold 9: 83.0\n",
            "Accuracy of fold 10: 80.0\n",
            "Final performance average: 81.1\n",
            "\n",
            "Unigram + Bigram + Trigram\n",
            "Accuracy of fold 1: 77.0\n",
            "Accuracy of fold 2: 79.0\n",
            "Accuracy of fold 3: 83.0\n",
            "Accuracy of fold 4: 85.0\n",
            "Accuracy of fold 5: 80.5\n",
            "Accuracy of fold 6: 83.5\n",
            "Accuracy of fold 7: 83.5\n",
            "Accuracy of fold 8: 80.0\n",
            "Accuracy of fold 9: 82.5\n",
            "Accuracy of fold 10: 80.5\n",
            "Final performance average: 81.45\n",
            "\n",
            "Sign test between unigram and unigram+bigram: p_value = 0.7712977979137471160722421800\n",
            "the difference is not significant\n",
            "\n",
            "Sign test between unigram and unigram+bigram+trigram: p_value = 0.8405124276655436267484850072\n",
            "the difference is not significant\n",
            "\n",
            "Sign test between unigram+bigram and unigram+bigram+trigram: p_value = 0.9821609888541456792697619327\n",
            "the difference is not significant\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dVrGGArkrWoL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Q3.10: How many features does the BoW model have to take into account now? (0.5pt)\n",
        "How does this number compare (e.g., linear, square, cubed, exponential) to the number of features at (Q3.8)? \n",
        "\n",
        "Use the held-out training set once again for this.\n"
      ]
    },
    {
      "metadata": {
        "id": "_z8sAJeUrdtM",
        "colab_type": "code",
        "outputId": "f37766cc-f960-4cfa-f26c-b8b798935334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        }
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "classes = [\"POS\",\"NEG\"]\n",
        "neg_reviews =  reviews[0:1000]\n",
        "pos_reviews = reviews[1000:]\n",
        "train_set = neg_reviews[:900] + pos_reviews[:900]\n",
        "\n",
        "nb = NB_classifier(classes,1,False,3)\n",
        "nb.fit(train_set)\n",
        "print(\"Size of unigram vocabulary without stemming: {}\".format( len(nb.unigram_voc) ))\n",
        "print(\"Size of bigram vocabulary without stemming: {}\".format( len(nb.bigram_voc) ))\n",
        "print(\"Size of trigram vocabulary without stemming: {}\".format( len(nb.trigram_voc) ))\n",
        "without_stemming = [len(nb.unigram_voc),len(nb.bigram_voc),len(nb.trigram_voc)]\n",
        "\n",
        "nb = NB_classifier(classes,1,True,3)\n",
        "nb.fit(train_set)\n",
        "print(\"\\nSize of unigram vocabulary with stemming: {}\".format( len(nb.unigram_voc) ))\n",
        "print(\"Size of bigram vocabulary with stemming: {}\".format( len(nb.bigram_voc) ))\n",
        "print(\"Size of trigram vocabulary with stemming: {}\".format( len(nb.trigram_voc) ))\n",
        "with_stemming = [len(nb.unigram_voc),len(nb.bigram_voc),len(nb.trigram_voc)]\n",
        "\n",
        "x = [0,1,2]\n",
        "plt.plot(x,without_stemming,'lime',label=\"Line between without stemming results\")\n",
        "plt.plot(x,without_stemming,'bo',label=\"Without stemming amounts\")\n",
        "plt.plot(x,with_stemming,'ro',label=\"With stemming amounts\")\n",
        "plt.xticks(np.arange(3), (\"Unigram\",\"Unigram+Bigram\",\"Unigram+Bigram+Trigram\"))\n",
        "plt.legend()\n",
        "plt.ylabel('Amount of features',fontsize=13)\n",
        "plt.xlabel('Type of ngrams',fontsize=13)\n",
        "title_for_plot = \"Graph of different total amount of features between different ngrams combinations\"\n",
        "plt.title(title_for_plot,fontsize=15,fontweight=\"bold\")\n",
        "\n",
        "#From the plot it can be observed, that the number of features increases linearly with the addition of bigrams and trigrams at each step for both with and without stemming. \n",
        "#Furtheremore, with stemming has less amount of features, which makes sense as described earlier in Q3.8"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of unigram vocabulary without stemming: 45348\n",
            "Size of bigram vocabulary without stemming: 425684\n",
            "Size of trigram vocabulary without stemming: 945654\n",
            "\n",
            "Size of unigram vocabulary with stemming: 32404\n",
            "Size of bigram vocabulary with stemming: 380872\n",
            "Size of trigram vocabulary with stemming: 920788\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5,1,'Graph of different total amount of features between different ngrams combinations')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAloAAAFrCAYAAAAJlSd/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f152755e748>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "CHWKDL3YV6vh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machines (4pts)\n"
      ]
    },
    {
      "metadata": {
        "id": "hJSYhcVaoJGt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Though simple to understand, implement, and debug, one\n",
        "major problem with the Naive Bayes classifier is that its performance\n",
        "deteriorates (becomes skewed) when it is being used with features which\n",
        "are not independent (i.e., are correlated). Another popular classifier\n",
        "that doesn’t scale as well to big data, and is not as simple to debug as\n",
        "Naive Bayes, but that doesn’t assume feature independence is the Support\n",
        "Vector Machine (SVM) classifier.\n",
        "\n",
        "You can find more details about SVMs in Chapter 7 of Bishop: Pattern Recognition and Machine Learning.\n",
        "Other sources for learning SVM:\n",
        "* http://web.mit.edu/zoya/www/SVM.pdf\n",
        "* http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf\n",
        "* https://pythonprogramming.net/support-vector-machine-intro-machine-learning-tutorial/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Use the scikit-learn implementation of \n",
        "[SVM.](http://scikit-learn.org/stable/modules/svm.html) with the default parameters.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "0LnzNtQBV8gr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### (Q4.1): Train SVM and compare to Naive Bayes (2pt)\n",
        "\n",
        "Train an SVM classifier (sklearn.svm.LinearSVC) using your features. Compare the\n",
        "classification performance of the SVM classifier to that of the Naive\n",
        "Bayes classifier from (Q3.4) and report the numbers.\n",
        "Do cross validation and concatenate the predictions from all folds to compute the significance.  Are the results significantly better?\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "JBscui8Mvoz0",
        "colab_type": "code",
        "outputId": "6719d81e-9f94-4641-e545-3af1fce79af3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "def get_FeaturesAndTargets(data,include_pos_tag=False,given_vocabulary=None,preference_pos=False):\n",
        "  targets = [] \n",
        "  corpus = []\n",
        "  desired_pos_tags = [\"N\",\"V\",\"J\",\"RB\"]\n",
        "  \n",
        "  for document in data:\n",
        "      targets.append(document[\"sentiment\"])\n",
        "      content = document[\"content\"]\n",
        "      \n",
        "      concat_full_text = \"\"\n",
        "      for sentence in content:\n",
        "         for token,pos_tag in sentence:\n",
        "            if include_pos_tag:\n",
        "                if not preference_pos:\n",
        "                  concat_full_text += token + pos_tag + \" \"\n",
        "                else:\n",
        "                   for desired_tag in desired_pos_tags: #The desired tags are like super classes, the pos tags can be JJ for example and still be part of J.\n",
        "                      if desired_tag in pos_tag:\n",
        "                        concat_full_text += token + pos_tag + \" \"\n",
        "                        break\n",
        "            else: \n",
        "                concat_full_text += token + \" \" #No need to lower() the token it as in CountVectorizer it already uses that\n",
        "      corpus.append(concat_full_text)\n",
        "  \n",
        "  if given_vocabulary == None:\n",
        "      vectorizer = CountVectorizer()\n",
        "      features = vectorizer.fit_transform(corpus)\n",
        "      vocabulary = vectorizer.vocabulary_ \n",
        "  else: \n",
        "      vectorizer = CountVectorizer(vocabulary=given_vocabulary)\n",
        "      features = vectorizer.transform(corpus)\n",
        "      vocabulary = given_vocabulary\n",
        "  \n",
        "  return features,targets,vocabulary\n",
        "\n",
        "classes = [\"POS\",\"NEG\"]\n",
        "neg_reviews =  reviews[0:1000]\n",
        "pos_reviews = reviews[1000:]\n",
        "train_set = neg_reviews[:900] + pos_reviews[:900]\n",
        "test_set = neg_reviews[900:] + pos_reviews[900:]\n",
        "\n",
        "#Use the same vocabulary to make it fair between the comparison. \n",
        "nb = NB_classifier(classes)\n",
        "nb.fit(train_set)\n",
        "naive_bayes_vocabulary = nb.unigram_voc\n",
        "\n",
        "modelSVM = LinearSVC()\n",
        "SVM_folds_performance, SVM_mean_acc, SVM_cv_results = cross_validation(modelSVM,reviews,10,True,vocabulary = naive_bayes_vocabulary)\n",
        "\n",
        "\n",
        "print(\"\\nSign test between smoothed unigram from Q3.4 and SVM: p_value =\\n\", p_value)\n",
        "p_value = sign_test(uni_smoothing_cv_results, SVM_cv_results)\n",
        "\n",
        "#document_term_matrix,targets,voc = get_FeaturesAndTargets(train_set,naive_bayes_vocabulary)\n",
        "#x_train = document_term_matrix\n",
        "#y_train = targets\n",
        "#model = LinearSVC()\n",
        "#model.fit(x_train,y_train)\n",
        "\n",
        "#from sklearn.metrics import accuracy_score\n",
        "#x_test,y_test,voc = get_FeaturesAndTargets(test_set,voc)\n",
        "#y_pred = model.predict(x_test)\n",
        "#accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of fold 1: 81.5\n",
            "Accuracy of fold 2: 82.0\n",
            "Accuracy of fold 3: 80.0\n",
            "Accuracy of fold 4: 86.0\n",
            "Accuracy of fold 5: 84.0\n",
            "Accuracy of fold 6: 82.5\n",
            "Accuracy of fold 7: 85.0\n",
            "Accuracy of fold 8: 85.5\n",
            "Accuracy of fold 9: 85.0\n",
            "Accuracy of fold 10: 81.0\n",
            "Final performance average: 83.25\n",
            "\n",
            "Sign test between smoothed unigram from Q3.4 and SVM: p_value =\n",
            " 0.7712977979137471160722421800\n",
            "the difference is not significant\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ifXVWcK0V9qY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### More linguistics\n",
        "\n",
        "Now add in part-of-speech features. You will find the\n",
        "movie review dataset has already been POS-tagged for you. Try to\n",
        "replicate what Pang et al. were doing:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "xA3I82o4oWGu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####(Q4.2) Replace your features with word+POS features, and report performance with the SVM. Does this help? Do cross validation and concatenate the predictions from all folds to compute the significance. Are the results significant? Why?  (1pt)\n"
      ]
    },
    {
      "metadata": {
        "id": "NOvjYe-t2Br6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "6d1a3735-f84c-4aa7-c235-bbea78aa7572"
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "classes = [\"POS\",\"NEG\"]\n",
        "neg_reviews =  reviews[0:1000]\n",
        "pos_reviews = reviews[1000:]\n",
        "train_set = neg_reviews[:900] + pos_reviews[:900]\n",
        "test_set = neg_reviews[900:] + pos_reviews[900:]\n",
        "\n",
        "#Don't have to use same vocabulary, make new one\n",
        "modelSVM = LinearSVC()\n",
        "SVM_POS_folds_performance, SVM_POS_mean_acc, SVM_POS_cv_results = cross_validation(modelSVM,reviews,10,True,include_pos_tag=True)\n",
        "\n",
        "#document_term_matrix,targets,voc = get_FeaturesAndTargets(train_set,True)\n",
        "#x_train = document_term_matrix\n",
        "#y_train = targets\n",
        "#model = LinearSVC()\n",
        "#model.fit(x_train,y_train)\n",
        "\n",
        "#from sklearn.metrics import accuracy_score\n",
        "#x_test,y_test,voc = get_FeaturesAndTargets(test_set,True,voc)\n",
        "#y_pred = model.predict(x_test)\n",
        "#accuracy_score(y_test, y_pred)\n",
        "print(\"\\nSign test between smoothed unigram from Q3.4 and SVM with POS tag: p_value =\\n\", p_value)\n",
        "p_value = sign_test(uni_smoothing_cv_results, SVM_POS_cv_results)\n",
        "\n",
        "print(\"\\nSign test between SVM without POS tag and SVM with POS tag: p_value =\\n\", p_value)\n",
        "p_value = sign_test(SVM_smoothing_cv_results, SVM_POS_cv_results)\n",
        "\n",
        "#It does seem to help as the results seem higher, but the sign test proves that the difference is not significant. "
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of fold 1: 81.5\n",
            "Accuracy of fold 2: 82.5\n",
            "Accuracy of fold 3: 83.5\n",
            "Accuracy of fold 4: 84.5\n",
            "Accuracy of fold 5: 83.5\n",
            "Accuracy of fold 6: 83.5\n",
            "Accuracy of fold 7: 85.5\n",
            "Accuracy of fold 8: 86.0\n",
            "Accuracy of fold 9: 85.5\n",
            "Accuracy of fold 10: 83.0\n",
            "Final performance average: 83.9\n",
            "\n",
            "Sign test between smoothed unigram from Q3.4 and SVM with POS tag: p_value =\n",
            " 0.4605822651825517471808702509\n",
            "the difference is not significant\n",
            "\n",
            "Sign test between SVM without POS tag and SVM with POS tag: p_value =\n",
            " 0.2932785698870256137209886960\n",
            "the difference is not significant\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Su-3w87eMW0w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### (Q4.3) Discard all closed-class words from your data (keep only nouns (N*), verbs (V*), adjectives (J*) and adverbs (RB*)), and report performance. Does this help? Do cross validation and concatenate the predictions from all folds to compute the significance. Are the results significantly better than when we don't discard the closed-class words? Why? (1pt)"
      ]
    },
    {
      "metadata": {
        "id": "CCUPlPozCYUX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "52f13066-40af-4421-ea37-0c6f868c6be4"
      },
      "cell_type": "code",
      "source": [
        "classes = [\"POS\",\"NEG\"]\n",
        "neg_reviews =  reviews[0:1000]\n",
        "pos_reviews = reviews[1000:]\n",
        "train_set = neg_reviews[:900] + pos_reviews[:900]\n",
        "test_set = neg_reviews[900:] + pos_reviews[900:]\n",
        "\n",
        "modelSVM = LinearSVC()\n",
        "SVM_closed_POS_folds_performance, SVM_closed_POS_mean_acc, SVM_closed_POS_cv_results = cross_validation(modelSVM,reviews,10,True,include_pos_tag=True,preference_pos=True)\n",
        "\n",
        "print(\"\\nSign test between smoothed unigram from Q3.4 and SVM with closed POS tag: p_value =\\n\", p_value)\n",
        "p_value = sign_test(uni_smoothing_cv_results, SVM_closed_POS_cv_results)\n",
        "\n",
        "print(\"\\nSign test between SVM without POS tag and SVM with POS tag: p_value =\\n\", p_value)\n",
        "p_value = sign_test(SVM_smoothing_cv_results, SVM_closed_POS_cv_results)\n",
        "\n",
        "print(\"\\nSign test between SVM with all POS tag and SVM with closed POS tag: p_value =\\n\", p_value)\n",
        "p_value = sign_test(SVM_POS_cv_results, SVM_closed_POS_cv_results)\n",
        "\n",
        "#The average accuracy is increased with 10 folds. Probably because the most important information are captured with the closed-class words, while the other are less important and act like noise. "
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of fold 1: 82.5\n",
            "Accuracy of fold 2: 84.5\n",
            "Accuracy of fold 3: 81.5\n",
            "Accuracy of fold 4: 87.0\n",
            "Accuracy of fold 5: 84.5\n",
            "Accuracy of fold 6: 83.0\n",
            "Accuracy of fold 7: 87.0\n",
            "Accuracy of fold 8: 82.5\n",
            "Accuracy of fold 9: 86.5\n",
            "Accuracy of fold 10: 81.5\n",
            "Final performance average: 84.05\n",
            "\n",
            "Sign test between smoothed unigram from Q3.4 and SVM with closed POS tag: p_value =\n",
            " 0.7712977979137471160722421800\n",
            "the difference is not significant\n",
            "\n",
            "Sign test between SVM without POS tag and SVM with POS tag: p_value =\n",
            " 0.2732177720293096189389068499\n",
            "the difference is not significant\n",
            "\n",
            "Sign test between SVM with all POS tag and SVM with closed POS tag: p_value =\n",
            " 0.7373250398720036864195787185\n",
            "the difference is not significant\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nfwqOciAl2No",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# (Q5) Discussion (max. 500 words). (5pts)\n",
        "\n",
        "> Based on your experiments, what are the effective features and techniques in sentiment analysis? What information do different features encode?\n",
        "Why is this important? What are the limitations of these features and techniques?\n",
        " \n"
      ]
    },
    {
      "metadata": {
        "id": "ZYuse5WLmekZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Write your answer here in max. 500 words.*"
      ]
    },
    {
      "metadata": {
        "id": "iwaKwfWQhRk_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Submission \n"
      ]
    },
    {
      "metadata": {
        "id": "aOUeaET5ijk-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Write your names and student numbers here:\n",
        "# Student 1 #12345\n",
        "# Student 2 #12345"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3A9K-H6Tii3X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**That's it!**\n",
        "\n",
        "- Check if you answered all questions fully and correctly. \n",
        "- Download your completed notebook using `File -> Download .ipynb` \n",
        "- Also save your notebook as a Github Gist. Get it by choosing `File -> Save as Github Gist`.  Make sure that the gist has a secret link (not public).\n",
        "- Check if your answers are all included in the file you submit (e.g. check the Github Gist URL)\n",
        "- Submit your .ipynb file and link to the Github Gist via *Canvas*. One submission per group. "
      ]
    }
  ]
}